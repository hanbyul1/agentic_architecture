{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ba3f58-a7f0-49d6-85b9-64eed17a2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agentic][Dynamic Routing][Training] Epoch 1 | CE: 1.005 | LB: 0.017 | Router: 1.002 | Entropy: 1.035\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [31456, 33178, 26232, 29134]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.24964860081672668, 0.24977873265743256, 0.2515988051891327, 0.24897359311580658]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 2 | CE: 0.636 | LB: 0.031 | Router: 0.636 | Entropy: 0.678\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [29073, 31942, 28929, 30056]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.25011909008026123, 0.2484709769487381, 0.25215843319892883, 0.24925167858600616]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 3 | CE: 0.491 | LB: 0.038 | Router: 0.492 | Entropy: 0.533\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [29002, 31358, 29413, 30227]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.2504115700721741, 0.24797096848487854, 0.2525334656238556, 0.24908433854579926]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 4 | CE: 0.408 | LB: 0.043 | Router: 0.409 | Entropy: 0.447\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [28859, 31035, 29707, 30399]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.2507985830307007, 0.24809104204177856, 0.25243085622787476, 0.2486797720193863]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 5 | CE: 0.351 | LB: 0.046 | Router: 0.352 | Entropy: 0.388\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [28832, 30880, 29872, 30416]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.25103309750556946, 0.24799370765686035, 0.2523420453071594, 0.24863098561763763]\n",
      "\n",
      "[Agentic][Dynamic Routing] Confusion Matrix: rows=True class, cols=Routed agent\n",
      "tensor([[1591,   86,  151,   72],\n",
      "        [  88, 1687,   60,   65],\n",
      "        [ 101,   53, 1564,  182],\n",
      "        [  98,   77,  208, 1517]])\n",
      "[Agentic][Dynamic Routing][Testing] Per-agent accuracy (handled samples): [0.8509052183173589, 0.8859695218076721, 0.7876954109934443, 0.8197167755991286]\n",
      "[Agentic][Dynamic Routing][Testing] Final routing distribution: {0: 1878, 1: 1903, 2: 1983, 3: 1836}\n",
      "[Agentic][Dynamic Routing][Testing] Overall test accuracy: 0.8356578947368422\n",
      "\n",
      "[Agentic][Static Routing] Agent outputs BEFORE:\n",
      "[Agentic] Agent 0 output: [[ 0.94936514 -3.7583616   0.76789236 -0.08191518]]\n",
      "[Agentic] Agent 1 output: [[ 0.76868474 -1.1102104   0.00684002 -0.90938175]]\n",
      "[Agentic] Agent 2 output: [[ 0.9148152  -3.0857666   1.1114038  -0.12984109]]\n",
      "[Agentic] Agent 3 output: [[ 0.8322148  -3.2972004   0.9743958   0.76907444]]\n",
      "\n",
      "[Agentic][Static Routing][Training] Agent 0 fine-tuning ...\n",
      "[Agentic][Static Routing][Agent 0 Fine-Tuning] Epoch 1 | Loss: 147.853\n",
      "[Agentic][Static Routing][Agent 0 Fine-Tuning] Epoch 2 | Loss: 0.000\n",
      "[Agentic][Static Routing][Agent 0 Fine-Tuning] Epoch 3 | Loss: 0.000\n",
      "\n",
      "[Agentic][Static Routing] Agent outputs AFTER agent 0 fine-tuning:\n",
      "[Agentic] Agent 0 output: [[ 13.440618 -17.625978 -13.139613 -13.247654]]\n",
      "[Agentic] Agent 1 output: [[ 0.76868474 -1.1102104   0.00684002 -0.90938175]]\n",
      "[Agentic] Agent 2 output: [[ 0.9148152  -3.0857666   1.1114038  -0.12984109]]\n",
      "[Agentic] Agent 3 output: [[ 0.8322148  -3.2972004   0.9743958   0.76907444]]\n",
      "[Agentic][Static Routing][Testing] Per-agent accuracy (handled samples): [1.0, 0.8936842105263157, 0.8336842105263158, 0.7863157894736842]\n",
      "\n",
      "[Agentic][CAC][Parallel Aggregation] For input X_test[2], aggregate all 4 agents (Majority Voting & Softmax Mean):\n",
      "[Agentic][CAC] Individual Agent 0 output: tensor([[ 10.1912, -14.6102, -12.6156, -12.5604]])\n",
      "[Agentic][CAC] Individual Agent 1 output: tensor([[-0.9425, -0.2614, -0.1471,  1.0282]])\n",
      "[Agentic][CAC] Individual Agent 2 output: tensor([[-1.4393, -1.3253,  0.1842,  1.2855]])\n",
      "[Agentic][CAC] Individual Agent 3 output: tensor([[-1.0327, -1.2637, -0.2171,  1.8019]])\n",
      "[Agentic][CAC] Mean (raw logits) predicted class: 0\n",
      "\n",
      "[Agentic][CAC][Learnable Coordinator] Dynamic selection for input X_test[2]:\n",
      "\n",
      "[Agentic][CAC][Learnable Coordinator] Selecting agents dynamically...\n",
      "[Agentic][CAC][SharedMemory] Updating memory for agent 1.\n",
      "[Agentic][CAC][SharedMemory] Updating memory for agent 3.\n",
      "[Agentic][CAC] Aggregated output (learnable selection): tensor([[[-0.9876, -0.7625, -0.1821,  1.4150]]])\n",
      "[Agentic] Agents selected by coordinator: [1, 3]\n",
      "[Agentic][CAC] Selected Agent 1 output: tensor([[-0.9425, -0.2614, -0.1471,  1.0282]])\n",
      "[Agentic][CAC] Selected Agent 3 output: tensor([[-1.0327, -1.2637, -0.2171,  1.8019]])\n",
      "[Agentic][CAC] Mean (raw logits) predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load AG News data: train and test splits\n",
    "train_dataset = load_dataset('ag_news', split='train')\n",
    "test_dataset  = load_dataset('ag_news', split='test')\n",
    "\n",
    "# 2. Build vocab on *training data only*\n",
    "tokenizer = lambda s: s.lower().split()\n",
    "vocab = build_vocab_from_iterator((tokenizer(x['text']) for x in train_dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 3. Encode samples (train & test separately)\n",
    "def encode(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return torch.tensor([vocab[token] for token in tokens][:8], dtype=torch.long)  # seq_len=8\n",
    "\n",
    "X_train = [encode(sample['text']) for sample in train_dataset]\n",
    "X_train = pad_sequence(X_train, batch_first=True, padding_value=0)\n",
    "y_train = torch.tensor([sample['label'] for sample in train_dataset])\n",
    "\n",
    "X_test = [encode(sample['text']) for sample in test_dataset]\n",
    "X_test = pad_sequence(X_test, batch_first=True, padding_value=0)\n",
    "y_test = torch.tensor([sample['label'] for sample in test_dataset])\n",
    "\n",
    "n_samples_train = len(X_train)\n",
    "n_samples_test  = len(X_test)\n",
    "\n",
    "# 4. Model setup (as in your code, unchanged)\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 10, model_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :x.size(1)]\n",
    "        out = self.encoder(x)\n",
    "        return out[:, 0, :]\n",
    "        \n",
    "class AgentFFN(nn.Module):\n",
    "    def __init__(self, model_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n",
    "class RoutingNetwork(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(model_dim, n_agents)\n",
    "    def forward(self, features):\n",
    "        logits = self.linear(features)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return logits, probs\n",
    "\n",
    "class AssignmentModule:\n",
    "    def __init__(self, n_agents):\n",
    "        self.n_agents = n_agents\n",
    "    def __call__(self, user_id):\n",
    "        if isinstance(user_id, torch.Tensor):\n",
    "            return (user_id % self.n_agents).item() \n",
    "        else:\n",
    "            return user_id % self.n_agents\n",
    "\n",
    "class DualRoutingModule(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents, agents):\n",
    "        super().__init__()\n",
    "        self.routing_network = RoutingNetwork(model_dim, n_agents)\n",
    "        self.assignment_module = AssignmentModule(n_agents)\n",
    "        self.agents = agents\n",
    "    def forward(self, features, user_id=None, mode='dynamic', return_routing=False):\n",
    "        batch_size = features.size(0)\n",
    "        outputs = []\n",
    "        if mode == 'dynamic':\n",
    "            logits, probs = self.routing_network(features)\n",
    "            agent_indices = torch.argmax(probs, dim=-1)\n",
    "            for i in range(batch_size):\n",
    "                ai = agent_indices[i].item()\n",
    "                outputs.append(self.agents[ai](features[i:i+1]))\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            if return_routing:\n",
    "                return outputs, logits, probs\n",
    "            else:\n",
    "                return outputs\n",
    "        elif mode == 'static':\n",
    "            assert user_id is not None, \"user_id required for static routing\"\n",
    "            agent_idx = self.assignment_module(user_id)\n",
    "            out = self.agents[agent_idx](features)\n",
    "            return out\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'dynamic' or 'static'\")\n",
    "\n",
    "class AgenticTransformerDualRouting(nn.Module):\n",
    "    def __init__(self, n_agents, vocab_size, model_dim, out_dim, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone(vocab_size, model_dim, n_heads=n_heads)\n",
    "        agents = nn.ModuleList([AgentFFN(model_dim, out_dim) for _ in range(n_agents)])\n",
    "        self.dual_routing_module = DualRoutingModule(model_dim, n_agents, agents)\n",
    "    def forward(self, x, user_id=None, mode='dynamic', return_routing=False):\n",
    "        shared = self.backbone(x)\n",
    "        return self.dual_routing_module(shared, user_id=user_id, mode=mode, return_routing=return_routing)\n",
    "\n",
    "class LearnableCoordinator(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents, n_select=2):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.n_select = n_select\n",
    "        self.selector = nn.Linear(model_dim, n_agents)  # Takes backbone features\n",
    "\n",
    "    def forward(self, features, agent_ids):\n",
    "        # features: (batch, model_dim) or (model_dim,) if batch=1\n",
    "        if features.dim() == 1:\n",
    "            features = features.unsqueeze(0)\n",
    "        logits = self.selector(features)  # (batch, n_agents)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Select top-k agents for each input in the batch\n",
    "        selected_indices = torch.topk(probs, self.n_select, dim=-1).indices\n",
    "        # Convert indices to agent IDs for each batch item\n",
    "        selected_agents = []\n",
    "        for i in range(features.size(0)):\n",
    "            selected_agents.append([agent_ids[j] for j in selected_indices[i].tolist()])\n",
    "        return selected_agents  # list of list of agent IDs (per batch)\n",
    "\n",
    "class CAC:\n",
    "    def __init__(self, model, coordinator=None, workflow=None):\n",
    "        self.model = model\n",
    "        self.shared_memory = {}\n",
    "        self.coordinator = coordinator      # Should be a callable (e.g., a neural net)\n",
    "        self.workflow = workflow            # List of agent IDs (workflow order)\n",
    "\n",
    "    def communicate(self, outputs, protocol=None):\n",
    "        communicated = []\n",
    "        for idx, out in enumerate(outputs):\n",
    "            # Example metadata: agent index, completion status, dummy confidence\n",
    "            metadata = {\n",
    "                'agent_idx': idx,\n",
    "                'status': 'complete',\n",
    "                'confidence': float(torch.rand(1))  # Simulated confidence\n",
    "            }\n",
    "            # Structured message (could be JSON serializable)\n",
    "            message = {\n",
    "                'output': out,\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            # Example rule: Only share if confidence > threshold (can set via protocol)\n",
    "            threshold = protocol.get('confidence_threshold', 0.0) if protocol else 0.0\n",
    "            if metadata['confidence'] > threshold:\n",
    "                print(f\"[Agentic][CAC][Communicate] Sharing Agent {idx} output with confidence {metadata['confidence']:.2f}\")\n",
    "                communicated.append({'output': out, 'metadata': {'agent_idx': idx}})\n",
    "            else:\n",
    "                print(f\"[Agentic][CAC][Communicate] Agent {idx} output NOT shared (confidence {metadata['confidence']:.2f})\")\n",
    "        # Return only the outputs for downstream processing\n",
    "        return [msg['output'] for msg in communicated]\n",
    "\n",
    "    def update_shared_memory(self, agent_id, data):\n",
    "        print(f\"[Agentic][CAC][SharedMemory] Updating memory for agent {agent_id}.\")\n",
    "        self.shared_memory[agent_id] = data\n",
    "\n",
    "    def forward(self, x, agent_ids, user_id=None, mode='static', aggregation='mean'):\n",
    "        # Get features from frozen backbone\n",
    "        with torch.no_grad():\n",
    "            features = self.model.backbone(x)  # (batch, model_dim)\n",
    "        if self.coordinator is not None:\n",
    "            print(\"\\n[Agentic][CAC][Learnable Coordinator] Selecting agents dynamically...\")\n",
    "            selected_agents = self.coordinator(features, agent_ids)\n",
    "        else:\n",
    "            selected_agents = [agent_ids for _ in range(x.size(0))]  # fallback: all agents\n",
    "\n",
    "        # xs, agent_ids = self.decompose_and_route(x, agent_ids)\n",
    "\n",
    "        outputs = []\n",
    "        # For each sample in batch\n",
    "        for i in range(x.size(0)):\n",
    "            sample_outputs = []\n",
    "            for agent_id in selected_agents[i]:\n",
    "                out = self.model.dual_routing_module.agents[agent_id](features[i].unsqueeze(0))\n",
    "                self.update_shared_memory(agent_id, out)\n",
    "                sample_outputs.append(out)\n",
    "            # Aggregate outputs for this sample (mean)\n",
    "            sample_agg = torch.mean(torch.stack(sample_outputs, dim=0), dim=0)\n",
    "            outputs.append(sample_agg)\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        return outputs        \n",
    "        \n",
    "\n",
    "# --------- 1. Model Initialization ---------\n",
    "n_agents = 4\n",
    "vocab_size = len(vocab)\n",
    "model_dim = 32\n",
    "out_dim = 4\n",
    "\n",
    "model = AgenticTransformerDualRouting(n_agents, vocab_size, model_dim, out_dim)\n",
    "clf_loss_fn = nn.CrossEntropyLoss()\n",
    "router_loss_fn = nn.CrossEntropyLoss()\n",
    "lb_lambda = 3\n",
    "router_lambda = 1.0\n",
    "entropy_lambda = 0.05\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# --------- 2. Joint Training (Dynamic Routing) ---------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_ce = 0\n",
    "    total_lb = 0\n",
    "    total_router = 0\n",
    "    total_entropy = 0\n",
    "    routing_counts = [0 for _ in range(n_agents)]\n",
    "    agent_probs_sum = torch.zeros(n_agents)\n",
    "    for batch_start in range(0, n_samples_train, batch_size):\n",
    "        X_batch = X_train[batch_start:batch_start+batch_size]\n",
    "        y_batch = y_train[batch_start:batch_start+batch_size]\n",
    "        out, logits, probs = model(X_batch, user_id=None, mode='dynamic', return_routing=True)\n",
    "        ce_loss = clf_loss_fn(out, y_batch)\n",
    "        router_loss = router_loss_fn(logits, y_batch)\n",
    "        probs_mean = probs.mean(dim=0)\n",
    "        lb_loss = ((probs_mean - 1.0/n_agents) ** 2).sum()\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).mean()\n",
    "        loss = ce_loss + router_lambda * router_loss + lb_lambda * lb_loss + entropy_lambda * entropy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_ce += ce_loss.item() * X_batch.size(0)\n",
    "        total_router += router_loss.item() * X_batch.size(0)\n",
    "        total_lb += lb_loss.item() * X_batch.size(0)\n",
    "        total_entropy += entropy.item() * X_batch.size(0)\n",
    "        routed = probs.argmax(dim=-1)\n",
    "        for idx in routed.tolist():\n",
    "            routing_counts[idx] += 1\n",
    "        agent_probs_sum += probs.sum(dim=0).detach()\n",
    "    print(f\"[Agentic][Dynamic Routing][Training] Epoch {epoch+1} | CE: {total_ce/n_samples_train:.3f} | LB: {total_lb/n_samples_train:.3f} | Router: {total_router/n_samples_train:.3f} | Entropy: {total_entropy/n_samples_train:.3f}\")\n",
    "    print(\"[Agentic][Dynamic Routing][Training] Hard assignment counts per agent:\", routing_counts)\n",
    "    print(\"[Agentic][Dynamic Routing][Training] Mean softmax probability per agent:\", (agent_probs_sum / n_samples_train).tolist())\n",
    "\n",
    "# --------- 3. Per-agent accuracy (using only samples routed to that agent). Evaluation on TEST SET (never used during training)\n",
    "def evaluate_per_agent_handled(model, X, y, handled_by, n_agents, mode):\n",
    "    per_agent_acc = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for agent in range(n_agents):\n",
    "            idxs = [i for i, assigned in enumerate(handled_by) if assigned == agent]\n",
    "            if len(idxs) == 0:\n",
    "                per_agent_acc.append(float('nan'))\n",
    "                continue\n",
    "            correct = 0\n",
    "            for i in idxs:\n",
    "                if mode == 'dynamic':\n",
    "                    out = model(X[i].unsqueeze(0), mode=mode)\n",
    "                else:\n",
    "                    out = model(X[i].unsqueeze(0), user_id=agent, mode=mode)\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                correct += int(pred == y[i].item())\n",
    "            per_agent_acc.append(correct / len(idxs))\n",
    "    return per_agent_acc\n",
    "\n",
    "# --------- 4. Confusion Matrix (True class vs Routed agent) ---------\n",
    "\n",
    "conf_mat = torch.zeros((n_agents, n_agents), dtype=torch.long)\n",
    "model.eval()\n",
    "all_routed = []\n",
    "with torch.no_grad():\n",
    "    for i in range(n_samples_test):\n",
    "        _, logits, probs = model(X_test[i].unsqueeze(0), return_routing=True)\n",
    "        routed_agent = probs.argmax(dim=-1).item()\n",
    "        conf_mat[y_test[i], routed_agent] += 1\n",
    "        all_routed.append(routed_agent)\n",
    "print(\"\\n[Agentic][Dynamic Routing] Confusion Matrix: rows=True class, cols=Routed agent\")\n",
    "print(conf_mat)\n",
    "\n",
    "per_agent_acc_dynamic = evaluate_per_agent_handled(model, X_test, y_test, all_routed, n_agents, mode='dynamic')\n",
    "print(\"[Agentic][Dynamic Routing][Testing] Per-agent accuracy (handled samples):\", per_agent_acc_dynamic)\n",
    "\n",
    "unique, counts = torch.tensor(all_routed).unique(return_counts=True)\n",
    "dist = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "print(f\"[Agentic][Dynamic Routing][Testing] Final routing distribution: {dist}\")\n",
    "\n",
    "# Test overall accuracy\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(n_samples_test):\n",
    "        out = model(X_test[i].unsqueeze(0), mode='dynamic')\n",
    "        pred = out.argmax(dim=1).item()\n",
    "        correct += int(pred == y_test[i].item())\n",
    "test_acc = correct / n_samples_test\n",
    "print(\"[Agentic][Dynamic Routing][Testing] Overall test accuracy:\", test_acc)\n",
    "\n",
    "# --------- 5. Post-Deployment: Freeze backbone and other agents ---------\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "target_agent = 0\n",
    "for i, agent in enumerate(model.dual_routing_module.agents):\n",
    "    if i != target_agent:\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# --------- 6. Independent Fine-Tuning for Agent 0 (Static Routing) ---------\n",
    "def print_agent_outputs(model, X, n_agents):\n",
    "    for agent_id in range(n_agents):\n",
    "        out = model(X[0].unsqueeze(0), user_id=agent_id, mode='static')\n",
    "        print(f\"[Agentic] Agent {agent_id} output: {out.detach().cpu().numpy()}\")\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing] Agent outputs BEFORE:\")\n",
    "print_agent_outputs(model, X_test, n_agents)  # Use test set to inspect agent outputs\n",
    "\n",
    "# ... Fine-tuning on agent 0 (pick data from train set)\n",
    "target_agent = 0\n",
    "idxs = (y_train == target_agent).nonzero(as_tuple=True)[0]\n",
    "X_new = X_train[idxs]\n",
    "y_new = y_train[idxs]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.dual_routing_module.agents[target_agent].parameters(), lr=5e-4)\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing][Training] Agent 0 fine-tuning ...\")\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_new)):\n",
    "        out = model(X_new[i].unsqueeze(0), user_id=target_agent, mode='static')\n",
    "        loss = clf_loss_fn(out, y_new[i].unsqueeze(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Agentic][Static Routing][Agent {target_agent} Fine-Tuning] Epoch {epoch+1} | Loss: {total_loss:.3f}\")\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing] Agent outputs AFTER agent 0 fine-tuning:\")\n",
    "print_agent_outputs(model, X_test, n_agents)  # Use test set for after\n",
    "\n",
    "def evaluate_per_agent_static_routing(model, X, y, n_agents):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for agent_id in range(n_agents):\n",
    "            idxs = (y == agent_id).nonzero(as_tuple=True)[0]\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in idxs:\n",
    "                out = model(X[i].unsqueeze(0), user_id=agent_id, mode='static')\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                correct += (pred == y[i].item())\n",
    "                total += 1\n",
    "            acc = correct / total if total > 0 else 0\n",
    "            results.append(acc)\n",
    "    return results\n",
    "\n",
    "# For static routing, each sample is assigned to its label as agent\n",
    "handled_by_static = [label.item() for label in y_test]\n",
    "per_agent_acc_static = evaluate_per_agent_handled(model, X_test, y_test, handled_by_static, n_agents, mode='static')\n",
    "print(\"[Agentic][Static Routing][Testing] Per-agent accuracy (handled samples):\", per_agent_acc_static)\n",
    "\n",
    "# --------- 7. CAC Scenario: Multi-agent Coordination Modes ---------\n",
    "\n",
    "# (A) Parallel aggregation (default)\n",
    "\n",
    "# Majority Voting aggregation\n",
    "def majority_vote(outputs):\n",
    "    preds = [out.argmax(dim=-1).item() for out in outputs]\n",
    "    # In case of ties, returns the smallest class\n",
    "    return max(set(preds), key=preds.count)\n",
    "\n",
    "# Softmax Mean aggregation\n",
    "def softmax_mean(outputs):\n",
    "    probs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    mean_probs = torch.mean(torch.stack(probs, dim=0), dim=0)\n",
    "    return mean_probs.argmax(dim=-1).item()\n",
    "    \n",
    "cac_parallel = CAC(model)\n",
    "\n",
    "input_example = X_test[2].unsqueeze(0)\n",
    "agents_to_coord = [0, 1, 2, 3]\n",
    "\n",
    "print(\"\\n[Agentic][CAC][Parallel Aggregation] For input X_test[2], aggregate all 4 agents (Majority Voting & Softmax Mean):\")\n",
    "# Get each agent's output\n",
    "agent_outputs_A = []\n",
    "for aid in agents_to_coord:\n",
    "    out = model(input_example, user_id=aid, mode='static')\n",
    "    agent_outputs_A.append(out)\n",
    "    print(f\"[Agentic][CAC] Individual Agent {aid} output: {out.detach()}\")\n",
    "agent_outputs_A = torch.cat(agent_outputs_A, dim=0)\n",
    "mean_output_A = agent_outputs_A.mean(dim=0)\n",
    "print(\"[Agentic][CAC] Mean (raw logits) predicted class:\", mean_output_A.argmax(dim=0).item())\n",
    "\n",
    "# (B) Learnable coordinator (random selection for illustration)\n",
    "# Freeze backbone and agents\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for agent in model.dual_routing_module.agents:\n",
    "    for param in agent.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define coordinator (to be trained)\n",
    "model_dim = 32\n",
    "n_agents = 4\n",
    "coordinator = LearnableCoordinator(model_dim, n_agents, n_select=2)\n",
    "cac = CAC(model, coordinator=coordinator)\n",
    "\n",
    "# Example batch\n",
    "# batch_X = X[:8]  # Batch of 8 samples\n",
    "agent_ids = list(range(n_agents))\n",
    "\n",
    "# Forward pass (agent selection is dynamic, learned)\n",
    "print(\"\\n[Agentic][CAC][Learnable Coordinator] Dynamic selection for input X_test[2]:\")\n",
    "agent_outputs_B = cac.forward(input_example, agent_ids=agent_ids, mode='static')\n",
    "print(f\"[Agentic][CAC] Aggregated output (learnable selection): {agent_outputs_B.detach()}\")\n",
    "\n",
    "# To see which agents were picked:\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(input_example)\n",
    "    selected_agents = coordinator(features, agent_ids)[0]\n",
    "    print(\"[Agentic] Agents selected by coordinator:\", selected_agents)\n",
    "    for aid in selected_agents:\n",
    "        out = model(input_example, user_id=aid, mode='static')\n",
    "        print(f\"[Agentic][CAC] Selected Agent {aid} output: {out.detach()}\")\n",
    "mean_output_B = agent_outputs_B.mean(dim=0)\n",
    "pred_class = mean_output_B.argmax().item()\n",
    "print(\"[Agentic][CAC] Mean (raw logits) predicted class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b60767-49d4-4676-8872-66b9e7efa7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crewai_env)",
   "language": "python",
   "name": "crewai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
