{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ba3f58-a7f0-49d6-85b9-64eed17a2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agentic][Dynamic Routing][Training] Epoch 1 | CE: 1.021 | LB: 0.016 | Router: 1.020 | Entropy: 1.054\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [30119, 32475, 27851, 29555]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.2492234706878662, 0.250166654586792, 0.25168219208717346, 0.24892814457416534]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 2 | CE: 0.657 | LB: 0.029 | Router: 0.658 | Entropy: 0.699\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [29138, 32039, 29284, 29539]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.24981167912483215, 0.24849776923656464, 0.2519057095050812, 0.24978488683700562]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 3 | CE: 0.511 | LB: 0.037 | Router: 0.512 | Entropy: 0.551\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [28979, 31317, 29687, 30017]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.2505224347114563, 0.2482551485300064, 0.25191405415534973, 0.24930773675441742]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 4 | CE: 0.425 | LB: 0.041 | Router: 0.426 | Entropy: 0.465\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [28964, 30995, 29814, 30227]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.25073641538619995, 0.24831712245941162, 0.2520734369754791, 0.24887268245220184]\n",
      "[Agentic][Dynamic Routing][Training] Epoch 5 | CE: 0.364 | LB: 0.045 | Router: 0.365 | Entropy: 0.402\n",
      "[Agentic][Dynamic Routing][Training] Hard assignment counts per agent: [28980, 30787, 29800, 30433]\n",
      "[Agentic][Dynamic Routing][Training] Mean softmax probability per agent: [0.25103217363357544, 0.24817417562007904, 0.2521546483039856, 0.24863873422145844]\n",
      "\n",
      "[Agentic][Dynamic Routing] Confusion Matrix: rows=True class, cols=Routed agent\n",
      "tensor([[1566,  115,  150,   69],\n",
      "        [  83, 1710,   62,   45],\n",
      "        [ 116,   62, 1553,  169],\n",
      "        [ 114,   92,  226, 1468]])\n",
      "[Agentic][Dynamic Routing][Testing] Per-agent accuracy (handled samples): [0.8355508249068654, 0.8630621526023244, 0.7810145655449523, 0.8372358652198744]\n",
      "[Agentic][Dynamic Routing][Testing] Final routing distribution: {0: 1879, 1: 1979, 2: 1991, 3: 1751}\n",
      "[Agentic][Dynamic Routing][Testing] Overall test accuracy: 0.8288157894736842\n",
      "\n",
      "[Agentic][Static Routing] Agent outputs BEFORE:\n",
      "[Agentic] Agent 0 output: [[ 0.35566643 -2.4901304   2.3163497  -1.3614193 ]]\n",
      "[Agentic] Agent 1 output: [[-1.0689708 -1.6332191  1.2721689 -1.6238804]]\n",
      "[Agentic] Agent 2 output: [[-0.19873013 -3.4652934   2.0715747  -2.078421  ]]\n",
      "[Agentic] Agent 3 output: [[-1.6423228 -3.4235642  1.4807295 -0.3250468]]\n",
      "\n",
      "[Agentic][Static Routing][Training] Agent 2 fine-tuning ...\n",
      "[Agentic][Static Routing][Agent 2 Fine-Tuning] Epoch 1 | Loss: 130.705\n",
      "[Agentic][Static Routing][Agent 2 Fine-Tuning] Epoch 2 | Loss: 0.000\n",
      "[Agentic][Static Routing][Agent 2 Fine-Tuning] Epoch 3 | Loss: 0.000\n",
      "\n",
      "[Agentic][Static Routing] Agent outputs AFTER agent 2 fine-tuning:\n",
      "[Agentic] Agent 0 output: [[ 0.35566643 -2.4901304   2.3163497  -1.3614193 ]]\n",
      "[Agentic] Agent 1 output: [[-1.0689708 -1.6332191  1.2721689 -1.6238804]]\n",
      "[Agentic] Agent 2 output: [[-13.005693 -17.111933  14.352101 -16.82693 ]]\n",
      "[Agentic] Agent 3 output: [[-1.6423228 -3.4235642  1.4807295 -0.3250468]]\n",
      "[Agentic][Static Routing][Testing] Per-agent accuracy (handled samples): [0.8205263157894737, 0.9036842105263158, 1.0, 0.783157894736842]\n",
      "\n",
      "[Agentic][CAC][Parallel Aggregation] For input X_test[2], aggregate all 4 agents (Majority Voting & Softmax Mean):\n",
      "[Agentic][CAC] Individual Agent 0 output: tensor([[-0.3453, -3.0997,  0.0818,  2.0384]])\n",
      "[Agentic][CAC] Individual Agent 1 output: tensor([[-1.0105, -1.1716, -0.7418,  1.5891]])\n",
      "[Agentic][CAC] Individual Agent 2 output: tensor([[-15.4214, -16.0471,  13.2530, -12.0009]])\n",
      "[Agentic][CAC] Individual Agent 3 output: tensor([[-1.7766, -3.6022,  0.9924,  2.2460]])\n",
      "[Agentic][CAC] Mean (raw logits) predicted class: 2\n",
      "\n",
      "[Agentic][CAC][Learnable Coordinator] Dynamic selection for input X_test[2]:\n",
      "\n",
      "[Agentic][CAC][Learnable Coordinator] Selecting agents dynamically...\n",
      "[Agentic][CAC][SharedMemory] Updating memory for agent 0.\n",
      "[Agentic][CAC][SharedMemory] Updating memory for agent 3.\n",
      "[Agentic][CAC] Aggregated output (learnable selection): tensor([[[-1.0610, -3.3510,  0.5371,  2.1422]]])\n",
      "[Agentic] Agents selected by coordinator: [0, 3]\n",
      "[Agentic][CAC] Selected Agent 0 output: tensor([[-0.3453, -3.0997,  0.0818,  2.0384]])\n",
      "[Agentic][CAC] Selected Agent 3 output: tensor([[-1.7766, -3.6022,  0.9924,  2.2460]])\n",
      "[Agentic][CAC] Mean (raw logits) predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load AG News data: train and test splits\n",
    "train_dataset = load_dataset('ag_news', split='train')\n",
    "test_dataset  = load_dataset('ag_news', split='test')\n",
    "\n",
    "# 2. Build vocab on *training data only*\n",
    "tokenizer = lambda s: s.lower().split()\n",
    "vocab = build_vocab_from_iterator((tokenizer(x['text']) for x in train_dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 3. Encode samples (train & test separately)\n",
    "def encode(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return torch.tensor([vocab[token] for token in tokens][:8], dtype=torch.long)  # seq_len=8\n",
    "\n",
    "X_train = [encode(sample['text']) for sample in train_dataset]\n",
    "X_train = pad_sequence(X_train, batch_first=True, padding_value=0)\n",
    "y_train = torch.tensor([sample['label'] for sample in train_dataset])\n",
    "\n",
    "X_test = [encode(sample['text']) for sample in test_dataset]\n",
    "X_test = pad_sequence(X_test, batch_first=True, padding_value=0)\n",
    "y_test = torch.tensor([sample['label'] for sample in test_dataset])\n",
    "\n",
    "n_samples_train = len(X_train)\n",
    "n_samples_test  = len(X_test)\n",
    "\n",
    "# 4. Model setup (as in your code, unchanged)\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 10, model_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :x.size(1)]\n",
    "        out = self.encoder(x)\n",
    "        return out[:, 0, :]\n",
    "        \n",
    "class AgentFFN(nn.Module):\n",
    "    def __init__(self, model_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n",
    "class RoutingNetwork(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(model_dim, n_agents)\n",
    "    def forward(self, features):\n",
    "        logits = self.linear(features)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return logits, probs\n",
    "\n",
    "class AssignmentModule:\n",
    "    def __init__(self, n_agents):\n",
    "        self.n_agents = n_agents\n",
    "    def __call__(self, user_id):\n",
    "        if isinstance(user_id, torch.Tensor):\n",
    "            return (user_id % self.n_agents).item() \n",
    "        else:\n",
    "            return user_id % self.n_agents\n",
    "\n",
    "class DualRoutingModule(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents, agents):\n",
    "        super().__init__()\n",
    "        self.routing_network = RoutingNetwork(model_dim, n_agents)\n",
    "        self.assignment_module = AssignmentModule(n_agents)\n",
    "        self.agents = agents\n",
    "    def forward(self, features, user_id=None, mode='dynamic', return_routing=False):\n",
    "        batch_size = features.size(0)\n",
    "        outputs = []\n",
    "        if mode == 'dynamic':\n",
    "            logits, probs = self.routing_network(features)\n",
    "            agent_indices = torch.argmax(probs, dim=-1)\n",
    "            for i in range(batch_size):\n",
    "                ai = agent_indices[i].item()\n",
    "                outputs.append(self.agents[ai](features[i:i+1]))\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            if return_routing:\n",
    "                return outputs, logits, probs\n",
    "            else:\n",
    "                return outputs\n",
    "        elif mode == 'static':\n",
    "            assert user_id is not None, \"user_id required for static routing\"\n",
    "            agent_idx = self.assignment_module(user_id)\n",
    "            out = self.agents[agent_idx](features)\n",
    "            return out\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'dynamic' or 'static'\")\n",
    "\n",
    "class AgenticTransformerDualRouting(nn.Module):\n",
    "    def __init__(self, n_agents, vocab_size, model_dim, out_dim, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone(vocab_size, model_dim, n_heads=n_heads)\n",
    "        agents = nn.ModuleList([AgentFFN(model_dim, out_dim) for _ in range(n_agents)])\n",
    "        self.dual_routing_module = DualRoutingModule(model_dim, n_agents, agents)\n",
    "    def forward(self, x, user_id=None, mode='dynamic', return_routing=False):\n",
    "        shared = self.backbone(x)\n",
    "        return self.dual_routing_module(shared, user_id=user_id, mode=mode, return_routing=return_routing)\n",
    "\n",
    "class LearnableCoordinator(nn.Module):\n",
    "    def __init__(self, model_dim, n_agents, n_select=2):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.n_select = n_select\n",
    "        self.selector = nn.Linear(model_dim, n_agents)  # Takes backbone features\n",
    "\n",
    "    def forward(self, features, agent_ids):\n",
    "        # features: (batch, model_dim) or (model_dim,) if batch=1\n",
    "        if features.dim() == 1:\n",
    "            features = features.unsqueeze(0)\n",
    "        logits = self.selector(features)  # (batch, n_agents)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Select top-k agents for each input in the batch\n",
    "        selected_indices = torch.topk(probs, self.n_select, dim=-1).indices\n",
    "        # Convert indices to agent IDs for each batch item\n",
    "        selected_agents = []\n",
    "        for i in range(features.size(0)):\n",
    "            selected_agents.append([agent_ids[j] for j in selected_indices[i].tolist()])\n",
    "        return selected_agents  # list of list of agent IDs (per batch)\n",
    "\n",
    "class CAC:\n",
    "    def __init__(self, model, coordinator=None, workflow=None):\n",
    "        self.model = model\n",
    "        self.shared_memory = {}\n",
    "        self.coordinator = coordinator      # Should be a callable (e.g., a neural net)\n",
    "        self.workflow = workflow            # List of agent IDs (workflow order)\n",
    "\n",
    "    def communicate(self, outputs, protocol=None):\n",
    "        communicated = []\n",
    "        for idx, out in enumerate(outputs):\n",
    "            # Example metadata: agent index, completion status, dummy confidence\n",
    "            metadata = {\n",
    "                'agent_idx': idx,\n",
    "                'status': 'complete',\n",
    "                'confidence': float(torch.rand(1))  # Simulated confidence\n",
    "            }\n",
    "            # Structured message (could be JSON serializable)\n",
    "            message = {\n",
    "                'output': out,\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            # Example rule: Only share if confidence > threshold (can set via protocol)\n",
    "            threshold = protocol.get('confidence_threshold', 0.0) if protocol else 0.0\n",
    "            if metadata['confidence'] > threshold:\n",
    "                print(f\"[Agentic][CAC][Communicate] Sharing Agent {idx} output with confidence {metadata['confidence']:.2f}\")\n",
    "                communicated.append({'output': out, 'metadata': {'agent_idx': idx}})\n",
    "            else:\n",
    "                print(f\"[Agentic][CAC][Communicate] Agent {idx} output NOT shared (confidence {metadata['confidence']:.2f})\")\n",
    "        # Return only the outputs for downstream processing\n",
    "        return [msg['output'] for msg in communicated]\n",
    "\n",
    "    def update_shared_memory(self, agent_id, data):\n",
    "        print(f\"[Agentic][CAC][SharedMemory] Updating memory for agent {agent_id}.\")\n",
    "        self.shared_memory[agent_id] = data\n",
    "\n",
    "    def forward(self, x, agent_ids, user_id=None, mode='static', aggregation='mean'):\n",
    "        # Get features from frozen backbone\n",
    "        with torch.no_grad():\n",
    "            features = self.model.backbone(x)  # (batch, model_dim)\n",
    "        if self.coordinator is not None:\n",
    "            print(\"\\n[Agentic][CAC][Learnable Coordinator] Selecting agents dynamically...\")\n",
    "            selected_agents = self.coordinator(features, agent_ids)\n",
    "        else:\n",
    "            selected_agents = [agent_ids for _ in range(x.size(0))]  # fallback: all agents\n",
    "\n",
    "        # xs, agent_ids = self.decompose_and_route(x, agent_ids)\n",
    "\n",
    "        outputs = []\n",
    "        # For each sample in batch\n",
    "        for i in range(x.size(0)):\n",
    "            sample_outputs = []\n",
    "            for agent_id in selected_agents[i]:\n",
    "                out = self.model.dual_routing_module.agents[agent_id](features[i].unsqueeze(0))\n",
    "                self.update_shared_memory(agent_id, out)\n",
    "                sample_outputs.append(out)\n",
    "            # Aggregate outputs for this sample (mean)\n",
    "            sample_agg = torch.mean(torch.stack(sample_outputs, dim=0), dim=0)\n",
    "            outputs.append(sample_agg)\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        return outputs        \n",
    "        \n",
    "\n",
    "# --------- 1. Model Initialization ---------\n",
    "n_agents = 4\n",
    "vocab_size = len(vocab)\n",
    "model_dim = 32\n",
    "out_dim = 4\n",
    "\n",
    "model = AgenticTransformerDualRouting(n_agents, vocab_size, model_dim, out_dim)\n",
    "clf_loss_fn = nn.CrossEntropyLoss()\n",
    "router_loss_fn = nn.CrossEntropyLoss()\n",
    "lb_lambda = 3\n",
    "router_lambda = 1.0\n",
    "entropy_lambda = 0.05\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# --------- 2. Joint Training (Dynamic Routing) ---------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_ce = 0\n",
    "    total_lb = 0\n",
    "    total_router = 0\n",
    "    total_entropy = 0\n",
    "    routing_counts = [0 for _ in range(n_agents)]\n",
    "    agent_probs_sum = torch.zeros(n_agents)\n",
    "    for batch_start in range(0, n_samples_train, batch_size):\n",
    "        X_batch = X_train[batch_start:batch_start+batch_size]\n",
    "        y_batch = y_train[batch_start:batch_start+batch_size]\n",
    "        out, logits, probs = model(X_batch, user_id=None, mode='dynamic', return_routing=True)\n",
    "        ce_loss = clf_loss_fn(out, y_batch)\n",
    "        router_loss = router_loss_fn(logits, y_batch)\n",
    "        probs_mean = probs.mean(dim=0)\n",
    "        lb_loss = ((probs_mean - 1.0/n_agents) ** 2).sum()\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).mean()\n",
    "        loss = ce_loss + router_lambda * router_loss + lb_lambda * lb_loss + entropy_lambda * entropy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_ce += ce_loss.item() * X_batch.size(0)\n",
    "        total_router += router_loss.item() * X_batch.size(0)\n",
    "        total_lb += lb_loss.item() * X_batch.size(0)\n",
    "        total_entropy += entropy.item() * X_batch.size(0)\n",
    "        routed = probs.argmax(dim=-1)\n",
    "        for idx in routed.tolist():\n",
    "            routing_counts[idx] += 1\n",
    "        agent_probs_sum += probs.sum(dim=0).detach()\n",
    "    print(f\"[Agentic][Dynamic Routing][Training] Epoch {epoch+1} | CE: {total_ce/n_samples_train:.3f} | LB: {total_lb/n_samples_train:.3f} | Router: {total_router/n_samples_train:.3f} | Entropy: {total_entropy/n_samples_train:.3f}\")\n",
    "    print(\"[Agentic][Dynamic Routing][Training] Hard assignment counts per agent:\", routing_counts)\n",
    "    print(\"[Agentic][Dynamic Routing][Training] Mean softmax probability per agent:\", (agent_probs_sum / n_samples_train).tolist())\n",
    "\n",
    "# --------- 3. Per-agent accuracy (using only samples routed to that agent). Evaluation on TEST SET (never used during training)\n",
    "def evaluate_per_agent_handled(model, X, y, handled_by, n_agents, mode):\n",
    "    per_agent_acc = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for agent in range(n_agents):\n",
    "            idxs = [i for i, assigned in enumerate(handled_by) if assigned == agent]\n",
    "            if len(idxs) == 0:\n",
    "                per_agent_acc.append(float('nan'))\n",
    "                continue\n",
    "            correct = 0\n",
    "            for i in idxs:\n",
    "                if mode == 'dynamic':\n",
    "                    out = model(X[i].unsqueeze(0), mode=mode)\n",
    "                else:\n",
    "                    out = model(X[i].unsqueeze(0), user_id=agent, mode=mode)\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                correct += int(pred == y[i].item())\n",
    "            per_agent_acc.append(correct / len(idxs))\n",
    "    return per_agent_acc\n",
    "\n",
    "# --------- 4. Confusion Matrix (True class vs Routed agent) ---------\n",
    "\n",
    "conf_mat = torch.zeros((n_agents, n_agents), dtype=torch.long)\n",
    "model.eval()\n",
    "all_routed = []\n",
    "with torch.no_grad():\n",
    "    for i in range(n_samples_test):\n",
    "        _, logits, probs = model(X_test[i].unsqueeze(0), return_routing=True)\n",
    "        routed_agent = probs.argmax(dim=-1).item()\n",
    "        conf_mat[y_test[i], routed_agent] += 1\n",
    "        all_routed.append(routed_agent)\n",
    "print(\"\\n[Agentic][Dynamic Routing] Confusion Matrix: rows=True class, cols=Routed agent\")\n",
    "print(conf_mat)\n",
    "\n",
    "per_agent_acc_dynamic = evaluate_per_agent_handled(model, X_test, y_test, all_routed, n_agents, mode='dynamic')\n",
    "print(\"[Agentic][Dynamic Routing][Testing] Per-agent accuracy (handled samples):\", per_agent_acc_dynamic)\n",
    "\n",
    "unique, counts = torch.tensor(all_routed).unique(return_counts=True)\n",
    "dist = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "print(f\"[Agentic][Dynamic Routing][Testing] Final routing distribution: {dist}\")\n",
    "\n",
    "# Test overall accuracy\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(n_samples_test):\n",
    "        out = model(X_test[i].unsqueeze(0), mode='dynamic')\n",
    "        pred = out.argmax(dim=1).item()\n",
    "        correct += int(pred == y_test[i].item())\n",
    "test_acc = correct / n_samples_test\n",
    "print(\"[Agentic][Dynamic Routing][Testing] Overall test accuracy:\", test_acc)\n",
    "\n",
    "# --------- 5. Post-Deployment: Freeze backbone and other agents ---------\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "target_agent = 2\n",
    "for i, agent in enumerate(model.dual_routing_module.agents):\n",
    "    if i != target_agent:\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# --------- 6. Independent Fine-Tuning for Agent 2 (Static Routing) ---------\n",
    "def print_agent_outputs(model, X, n_agents):\n",
    "    for agent_id in range(n_agents):\n",
    "        out = model(X[0].unsqueeze(0), user_id=agent_id, mode='static')\n",
    "        print(f\"[Agentic] Agent {agent_id} output: {out.detach().cpu().numpy()}\")\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing] Agent outputs BEFORE:\")\n",
    "print_agent_outputs(model, X_test, n_agents)  # Use test set to inspect agent outputs\n",
    "\n",
    "# ... Fine-tuning on agent 2 (pick data from train set)\n",
    "target_agent = 2\n",
    "idxs = (y_train == target_agent).nonzero(as_tuple=True)[0]\n",
    "X_new = X_train[idxs]\n",
    "y_new = y_train[idxs]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.dual_routing_module.agents[target_agent].parameters(), lr=5e-4)\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing][Training] Agent 2 fine-tuning ...\")\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_new)):\n",
    "        out = model(X_new[i].unsqueeze(0), user_id=target_agent, mode='static')\n",
    "        loss = clf_loss_fn(out, y_new[i].unsqueeze(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Agentic][Static Routing][Agent {target_agent} Fine-Tuning] Epoch {epoch+1} | Loss: {total_loss:.3f}\")\n",
    "\n",
    "print(\"\\n[Agentic][Static Routing] Agent outputs AFTER agent 2 fine-tuning:\")\n",
    "print_agent_outputs(model, X_test, n_agents)  # Use test set for after\n",
    "\n",
    "def evaluate_per_agent_static_routing(model, X, y, n_agents):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for agent_id in range(n_agents):\n",
    "            idxs = (y == agent_id).nonzero(as_tuple=True)[0]\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in idxs:\n",
    "                out = model(X[i].unsqueeze(0), user_id=agent_id, mode='static')\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                correct += (pred == y[i].item())\n",
    "                total += 1\n",
    "            acc = correct / total if total > 0 else 0\n",
    "            results.append(acc)\n",
    "    return results\n",
    "\n",
    "# For static routing, each sample is assigned to its label as agent\n",
    "handled_by_static = [label.item() for label in y_test]\n",
    "per_agent_acc_static = evaluate_per_agent_handled(model, X_test, y_test, handled_by_static, n_agents, mode='static')\n",
    "print(\"[Agentic][Static Routing][Testing] Per-agent accuracy (handled samples):\", per_agent_acc_static)\n",
    "\n",
    "# --------- 7. CAC Scenario: Multi-agent Coordination Modes ---------\n",
    "\n",
    "# (A) Parallel aggregation (default)\n",
    "\n",
    "# Majority Voting aggregation\n",
    "def majority_vote(outputs):\n",
    "    preds = [out.argmax(dim=-1).item() for out in outputs]\n",
    "    # In case of ties, returns the smallest class\n",
    "    return max(set(preds), key=preds.count)\n",
    "\n",
    "# Softmax Mean aggregation\n",
    "def softmax_mean(outputs):\n",
    "    probs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    mean_probs = torch.mean(torch.stack(probs, dim=0), dim=0)\n",
    "    return mean_probs.argmax(dim=-1).item()\n",
    "    \n",
    "cac_parallel = CAC(model)\n",
    "\n",
    "input_example = X_test[2].unsqueeze(0)\n",
    "agents_to_coord = [0, 1, 2, 3]\n",
    "\n",
    "print(\"\\n[Agentic][CAC][Parallel Aggregation] For input X_test[2], aggregate all 4 agents (Majority Voting & Softmax Mean):\")\n",
    "# Get each agent's output\n",
    "agent_outputs_A = []\n",
    "for aid in agents_to_coord:\n",
    "    out = model(input_example, user_id=aid, mode='static')\n",
    "    agent_outputs_A.append(out)\n",
    "    print(f\"[Agentic][CAC] Individual Agent {aid} output: {out.detach()}\")\n",
    "agent_outputs_A = torch.cat(agent_outputs_A, dim=0)\n",
    "mean_output_A = agent_outputs_A.mean(dim=0)\n",
    "print(\"[Agentic][CAC] Mean (raw logits) predicted class:\", mean_output_A.argmax(dim=0).item())\n",
    "\n",
    "# (B) Learnable coordinator (random selection for illustration)\n",
    "# Freeze backbone and agents\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for agent in model.dual_routing_module.agents:\n",
    "    for param in agent.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define coordinator (to be trained)\n",
    "model_dim = 32\n",
    "n_agents = 4\n",
    "coordinator = LearnableCoordinator(model_dim, n_agents, n_select=2)\n",
    "cac = CAC(model, coordinator=coordinator)\n",
    "\n",
    "# Example batch\n",
    "# batch_X = X[:8]  # Batch of 8 samples\n",
    "agent_ids = list(range(n_agents))\n",
    "\n",
    "# Forward pass (agent selection is dynamic, learned)\n",
    "print(\"\\n[Agentic][CAC][Learnable Coordinator] Dynamic selection for input X_test[2]:\")\n",
    "agent_outputs_B = cac.forward(input_example, agent_ids=agent_ids, mode='static')\n",
    "print(f\"[Agentic][CAC] Aggregated output (learnable selection): {agent_outputs_B.detach()}\")\n",
    "\n",
    "# To see which agents were picked:\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(input_example)\n",
    "    selected_agents = coordinator(features, agent_ids)[0]\n",
    "    print(\"[Agentic] Agents selected by coordinator:\", selected_agents)\n",
    "    for aid in selected_agents:\n",
    "        out = model(input_example, user_id=aid, mode='static')\n",
    "        print(f\"[Agentic][CAC] Selected Agent {aid} output: {out.detach()}\")\n",
    "mean_output_B = agent_outputs_B.mean(dim=0)\n",
    "pred_class = mean_output_B.argmax().item()\n",
    "print(\"[Agentic][CAC] Mean (raw logits) predicted class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b60767-49d4-4676-8872-66b9e7efa7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crewai_env)",
   "language": "python",
   "name": "crewai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
